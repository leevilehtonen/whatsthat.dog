{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Image Classification - Dog Breeds\n",
    "\n",
    "\n",
    "### Description\n",
    "This notebook is part of project **whatsthat.dog** which is website for identifying dog breed in a given image of a dog. The goal of this notebook is to export a trained model in .h5 (HDF5) format and additionally files for importing the model to JavaScript. \n",
    "\n",
    "### Datasets\n",
    "The model will be trained using the following datasets:\n",
    "- [ImageNet](http://image-net.org)\n",
    "- [Stanford Dogs Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/)\n",
    "\n",
    "### Model\n",
    "The initial plan is to use __NASNet model__ (https://arxiv.org/abs/1707.07012) pretrained with __ImageNet__ data and then modify the structure to have an additional dense layer and final classification layer (with nodes as many as dog breeds). Then the final model will be trained with the __Stanford Dogs Dataset__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start by defining all imports and global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re, time, os\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from tensorflow.python.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.applications import NASNetMobile\n",
    "from tensorflow.python.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Conv2D, MaxPool2D, BatchNormalization, GlobalAveragePooling2D\n",
    "\n",
    "\n",
    "RANDOM_STATE = 20\n",
    "NAME = f\"dogs_breeds_{int(time.time())}\"\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_CHANNELS = 3\n",
    "IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "INPUT_SHAPE = (*IMAGE_SIZE, IMAGE_CHANNELS)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 40\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "Lets begin by processing the data. The dataset has following structure:\n",
    "- Data\n",
    "    - **Images**\n",
    "        - Breed1\n",
    "            - Image1\n",
    "            - ...\n",
    "            - ImageN\n",
    "        - ...\n",
    "        - Breed120\n",
    "            - Image1\n",
    "            - ...\n",
    "            - ImageN\n",
    "    - **Annotation**\n",
    "        - Breed1\n",
    "            - Annotation1\n",
    "            - ...\n",
    "            - AnnotationN\n",
    "        - ...\n",
    "        - Breed120\n",
    "            - Annotation1\n",
    "            - ...\n",
    "            - AnnotationN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername_pattern = re.compile(r\"n\\d{8}-(.*)\")\n",
    "\n",
    "image_paths = []\n",
    "annotation_paths = []\n",
    "categories = []\n",
    "category_names = {}\n",
    "\n",
    "for index, folder in enumerate(os.listdir(\"data/Images\")):\n",
    "    \n",
    "    category = foldername_pattern.search(folder).groups()[0]\n",
    "    category_names[index] = category.replace(\"_\", \" \").capitalize()\n",
    "    \n",
    "    for item in os.listdir(f\"data/Images/{folder}\"):\n",
    "        \n",
    "        image_paths.append(f\"data/Images/{folder}/{item}\")\n",
    "        annotation_paths.append(f\"data/Annotation/{folder}/{item.split('.')[0]}\")\n",
    "        categories.append(index)\n",
    "                                \n",
    "df_input = pd.DataFrame({\"image_file\": image_paths, \"annotation_file\": annotation_paths, \"category\": categories})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataframe is now initialized which includes relative paths to all images and their annotations as well as to the respective label. Let's take a peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_file</th>\n",
       "      <th>annotation_file</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/Images/n02085620-Chihuahua/n02085620_1007...</td>\n",
       "      <td>data/Annotation/n02085620-Chihuahua/n02085620_...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/Images/n02085620-Chihuahua/n02085620_1013...</td>\n",
       "      <td>data/Annotation/n02085620-Chihuahua/n02085620_...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/Images/n02085620-Chihuahua/n02085620_1062...</td>\n",
       "      <td>data/Annotation/n02085620-Chihuahua/n02085620_...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/Images/n02085620-Chihuahua/n02085620_1073...</td>\n",
       "      <td>data/Annotation/n02085620-Chihuahua/n02085620_...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/Images/n02085620-Chihuahua/n02085620_1097...</td>\n",
       "      <td>data/Annotation/n02085620-Chihuahua/n02085620_...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_file  \\\n",
       "0  data/Images/n02085620-Chihuahua/n02085620_1007...   \n",
       "1  data/Images/n02085620-Chihuahua/n02085620_1013...   \n",
       "2  data/Images/n02085620-Chihuahua/n02085620_1062...   \n",
       "3  data/Images/n02085620-Chihuahua/n02085620_1073...   \n",
       "4  data/Images/n02085620-Chihuahua/n02085620_1097...   \n",
       "\n",
       "                                     annotation_file  category  \n",
       "0  data/Annotation/n02085620-Chihuahua/n02085620_...         0  \n",
       "1  data/Annotation/n02085620-Chihuahua/n02085620_...         0  \n",
       "2  data/Annotation/n02085620-Chihuahua/n02085620_...         0  \n",
       "3  data/Annotation/n02085620-Chihuahua/n02085620_...         0  \n",
       "4  data/Annotation/n02085620-Chihuahua/n02085620_...         0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_file</th>\n",
       "      <th>annotation_file</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20575</th>\n",
       "      <td>data/Images/n02116738-African_hunting_dog/n021...</td>\n",
       "      <td>data/Annotation/n02116738-African_hunting_dog/...</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20576</th>\n",
       "      <td>data/Images/n02116738-African_hunting_dog/n021...</td>\n",
       "      <td>data/Annotation/n02116738-African_hunting_dog/...</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20577</th>\n",
       "      <td>data/Images/n02116738-African_hunting_dog/n021...</td>\n",
       "      <td>data/Annotation/n02116738-African_hunting_dog/...</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20578</th>\n",
       "      <td>data/Images/n02116738-African_hunting_dog/n021...</td>\n",
       "      <td>data/Annotation/n02116738-African_hunting_dog/...</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20579</th>\n",
       "      <td>data/Images/n02116738-African_hunting_dog/n021...</td>\n",
       "      <td>data/Annotation/n02116738-African_hunting_dog/...</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              image_file  \\\n",
       "20575  data/Images/n02116738-African_hunting_dog/n021...   \n",
       "20576  data/Images/n02116738-African_hunting_dog/n021...   \n",
       "20577  data/Images/n02116738-African_hunting_dog/n021...   \n",
       "20578  data/Images/n02116738-African_hunting_dog/n021...   \n",
       "20579  data/Images/n02116738-African_hunting_dog/n021...   \n",
       "\n",
       "                                         annotation_file  category  \n",
       "20575  data/Annotation/n02116738-African_hunting_dog/...       119  \n",
       "20576  data/Annotation/n02116738-African_hunting_dog/...       119  \n",
       "20577  data/Annotation/n02116738-African_hunting_dog/...       119  \n",
       "20578  data/Annotation/n02116738-African_hunting_dog/...       119  \n",
       "20579  data/Annotation/n02116738-African_hunting_dog/...       119  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_input[\"category\"].value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can notice that the data is not distributed evenly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "The next step is to actually load in the data, normalize it and split it to train, validation and test data. Loading will be done using __Keras__ helper functions which utilizes the pillow library. \n",
    "\n",
    "For the data split I have chosen to include only 10% to the test data and 20% for validation and rest 70% for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20580/20580 [01:22<00:00, 248.60it/s]\n"
     ]
    }
   ],
   "source": [
    "df_input = df_input.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for index, row in tqdm(df_input.iterrows(), total=len(df_input)):\n",
    "    \n",
    "    img = load_img(row[\"image_file\"])\n",
    "    annotation_tree = ET.parse(row[\"annotation_file\"])\n",
    "    bndbox = {i.tag : int(i.text) for i in annotation_tree.getroot()[5][4]}   \n",
    "    img = img.crop((bndbox[\"xmin\"],bndbox[\"ymin\"], bndbox[\"xmax\"], bndbox[\"ymax\"]))\n",
    "    img = img.resize(IMAGE_SIZE)\n",
    "    \n",
    "    X.append(img_to_array(img))\n",
    "    y.append(row[\"category\"])\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X = X / 255.0\n",
    "\n",
    "split_distribution = [int(0.7*len(X)), int(0.9*len(X))]\n",
    "X_train, X_validation, X_test = np.split(X, split_distribution)\n",
    "y_train, y_validation, y_test = np.split(y, split_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is ready for the training and the model can be defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model\n",
    "As mentioned at the beginning the model will be based on the __NASnet__ architecture and more in detail to the __Mobile__ variant. This is to increase performance especially if the model is eventually deployed to browser or mobile environment. One of the reasons also for using the NASnet is that Keras library includes the model already pretrained with ImageNet dataset. Next step is to define the model and compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = NASNetMobile(weights=\"imagenet\", include_top=False, input_shape=INPUT_SHAPE)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dense(len(category_names), activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek for the underlying architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "NASNet (Model)               (None, 7, 7, 1056)        4269716   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1056)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              1082368   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               123000    \n",
      "=================================================================\n",
      "Total params: 5,475,084\n",
      "Trainable params: 1,205,368\n",
      "Non-trainable params: 4,269,716\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now both the data and the model is ready, and the training can be started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "To make training more smooth and allow analysis afterwards, callbacks are used, namely, Tensorboard, EarlyStopping and ReduceLROnPlateau. After defining these the training process can be started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = TensorBoard(log_dir=f\"logs/{NAME}\")\n",
    "earlystop_cb = EarlyStopping(patience=5)\n",
    "reducelronplateau_cb = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, verbose=1, factor=0.5, min_lr=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14405 samples, validate on 4117 samples\n",
      "Epoch 1/40\n",
      " - 109s - loss: 0.2472 - acc: 0.9168 - val_loss: 0.5683 - val_acc: 0.8419\n",
      "Epoch 2/40\n",
      " - 62s - loss: 0.2384 - acc: 0.9182 - val_loss: 0.5648 - val_acc: 0.8416\n",
      "Epoch 3/40\n",
      " - 59s - loss: 0.2364 - acc: 0.9188 - val_loss: 0.5645 - val_acc: 0.8428\n",
      "Epoch 4/40\n",
      " - 64s - loss: 0.2396 - acc: 0.9185 - val_loss: 0.5694 - val_acc: 0.8445\n",
      "Epoch 5/40\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      " - 65s - loss: 0.2303 - acc: 0.9244 - val_loss: 0.5683 - val_acc: 0.8407\n",
      "Epoch 6/40\n",
      " - 62s - loss: 0.2228 - acc: 0.9254 - val_loss: 0.5687 - val_acc: 0.8411\n",
      "Epoch 7/40\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      " - 60s - loss: 0.2260 - acc: 0.9234 - val_loss: 0.5701 - val_acc: 0.8421\n",
      "Epoch 8/40\n",
      " - 60s - loss: 0.2245 - acc: 0.9250 - val_loss: 0.5709 - val_acc: 0.8416\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = 'models/dogs_breeds_1558010569.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-a07c401b9cc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"models/{NAME}.h5\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\default-AI\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m    317\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_model\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m     \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\default-AI\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\default-AI\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\default-AI\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;31m# Open in append mode (read/write).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to create file (unable to open file: name = 'models/dogs_breeds_1558010569.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=X_train, \n",
    "    y=y_train, \n",
    "    validation_data=(X_validation, y_validation), \n",
    "    epochs=EPOCHS, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    callbacks=[tensorboard_cb, earlystop_cb, reducelronplateau_cb],\n",
    "    verbose=2\n",
    ")\n",
    "model.save(f\"models/{NAME}.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring the model\n",
    "After training the model is evaluated with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2058/2058 [==============================] - ETA: 9s - loss: 0.9446 - acc: 0.718 - ETA: 9s - loss: 0.7138 - acc: 0.796 - ETA: 8s - loss: 0.6766 - acc: 0.833 - ETA: 8s - loss: 0.6589 - acc: 0.828 - ETA: 8s - loss: 0.5623 - acc: 0.843 - ETA: 8s - loss: 0.5488 - acc: 0.838 - ETA: 8s - loss: 0.5297 - acc: 0.834 - ETA: 8s - loss: 0.6015 - acc: 0.820 - ETA: 8s - loss: 0.6089 - acc: 0.826 - ETA: 7s - loss: 0.6234 - acc: 0.828 - ETA: 7s - loss: 0.5954 - acc: 0.835 - ETA: 7s - loss: 0.5754 - acc: 0.838 - ETA: 7s - loss: 0.5950 - acc: 0.836 - ETA: 7s - loss: 0.5835 - acc: 0.839 - ETA: 7s - loss: 0.5854 - acc: 0.841 - ETA: 6s - loss: 0.6116 - acc: 0.839 - ETA: 6s - loss: 0.6189 - acc: 0.840 - ETA: 6s - loss: 0.6089 - acc: 0.843 - ETA: 6s - loss: 0.6081 - acc: 0.845 - ETA: 6s - loss: 0.5973 - acc: 0.843 - ETA: 6s - loss: 0.6105 - acc: 0.837 - ETA: 6s - loss: 0.6486 - acc: 0.833 - ETA: 6s - loss: 0.6391 - acc: 0.837 - ETA: 5s - loss: 0.6305 - acc: 0.838 - ETA: 5s - loss: 0.6239 - acc: 0.838 - ETA: 5s - loss: 0.6170 - acc: 0.841 - ETA: 5s - loss: 0.6391 - acc: 0.833 - ETA: 5s - loss: 0.6435 - acc: 0.834 - ETA: 5s - loss: 0.6333 - acc: 0.834 - ETA: 4s - loss: 0.6387 - acc: 0.832 - ETA: 4s - loss: 0.6453 - acc: 0.830 - ETA: 4s - loss: 0.6497 - acc: 0.832 - ETA: 4s - loss: 0.6646 - acc: 0.828 - ETA: 4s - loss: 0.6621 - acc: 0.826 - ETA: 4s - loss: 0.6624 - acc: 0.825 - ETA: 4s - loss: 0.6662 - acc: 0.824 - ETA: 3s - loss: 0.6739 - acc: 0.826 - ETA: 3s - loss: 0.6815 - acc: 0.825 - ETA: 3s - loss: 0.6705 - acc: 0.828 - ETA: 3s - loss: 0.6815 - acc: 0.828 - ETA: 3s - loss: 0.6910 - acc: 0.825 - ETA: 3s - loss: 0.6888 - acc: 0.824 - ETA: 3s - loss: 0.6819 - acc: 0.824 - ETA: 2s - loss: 0.6816 - acc: 0.825 - ETA: 2s - loss: 0.6775 - acc: 0.826 - ETA: 2s - loss: 0.6863 - acc: 0.824 - ETA: 2s - loss: 0.6888 - acc: 0.823 - ETA: 2s - loss: 0.6795 - acc: 0.826 - ETA: 2s - loss: 0.6822 - acc: 0.825 - ETA: 2s - loss: 0.6762 - acc: 0.823 - ETA: 1s - loss: 0.6794 - acc: 0.824 - ETA: 1s - loss: 0.6750 - acc: 0.825 - ETA: 1s - loss: 0.6721 - acc: 0.825 - ETA: 1s - loss: 0.6663 - acc: 0.826 - ETA: 1s - loss: 0.6617 - acc: 0.827 - ETA: 1s - loss: 0.6658 - acc: 0.828 - ETA: 1s - loss: 0.6604 - acc: 0.828 - ETA: 0s - loss: 0.6511 - acc: 0.831 - ETA: 0s - loss: 0.6476 - acc: 0.832 - ETA: 0s - loss: 0.6425 - acc: 0.833 - ETA: 0s - loss: 0.6388 - acc: 0.834 - ETA: 0s - loss: 0.6369 - acc: 0.835 - ETA: 0s - loss: 0.6428 - acc: 0.833 - ETA: 0s - loss: 0.6384 - acc: 0.834 - 9s 4ms/sample - loss: 0.6378 - acc: 0.8338\n",
      "Test loss: 0.6378188911749392\n",
      "Test accuracy: 0.83381927\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
